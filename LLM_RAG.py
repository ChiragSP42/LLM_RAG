# -*- coding: utf-8 -*-
"""LLM_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lIOin6rqxGBGec4OGmnuEwUyzsfSPZXi
"""

# from google.colab import drive
# drive.mount("/content/drive")

# !pip install flash-attn
# !pip install chromadb
# !pip install sentence_transformers

import os
import re
from dotenv import load_dotenv
import torch
import chromadb
# from chromadb.utils import embedding_functions
# from chromadb import EmbeddingFunction, Embeddings
# from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline
import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import huggingface_hub
# from huggingface_hub import login
# from google.colab import userdata
load_dotenv()
HF_TOKEN = os.getenv('HF_TOKEN')
os.environ["TOKENIZERS_PARALLELISM"] = "false"
huggingface_hub.login(token=HF_TOKEN, add_to_git_credential=True)

model_name = "intfloat/e5-base-v2"
# BERT MODEL AND TOKENIZER
bert_tokenizer = AutoTokenizer.from_pretrained(model_name)
bert_model = AutoModel.from_pretrained(model_name, output_hidden_states = True)
name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

def read_text_file(path: str) -> list[str]:
    """
    Read the text file

    Parameters:
    -----------

    path -> str: Path of text file

    Returns:
    ---------

    texts -> list[str]: Returns a list of sentences extracted from text file.
    """

    texts = []
    with open(path, 'r') as file:
        texts = file.readlines()

    return texts

# DRIVE_PATH = '/content/drive/MyDrive/Colab Notebooks'
DRIVE_PATH = os.getcwd()

TEXT_PATH = os.path.join(DRIVE_PATH, 'Stock_related_definitions.txt')
texts = read_text_file(TEXT_PATH)

CHROMA_DATA_PATH = "chroma_data/"
# EMBED_MODEL = "all-MiniLM-L6-v2"
COLLECTION_NAME = "stock_terms"
# chroma_embedding_model = SentenceTransformer("sentence-transformers/sentence-t5-base")
client = chromadb.PersistentClient(path=CHROMA_DATA_PATH)

# embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(
#     model_name=EMBED_MODEL
# )
try:
    collection = client.get_collection(name=COLLECTION_NAME)
except:
    print("Collection does not exist. First run vector_database_builder.py\
          to create collection with all stock related terms and use the \
          same name here.")
    
# Use a pipeline as a high-level helper
# tokenizer = AutoTokenizer.from_pretrained("tokenizer/", trust_remote_code=True)
# model = AutoModelForCausalLM.from_pretrained("model/", trust_remote_code=True)

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("apple/OpenELM-450M-Instruct", trust_remote_code=True)

while True:
  model.eval()
  question = input("Chat here: ")
  if question == 'Q':
      print("Bye")
      break
  context = collection.query(
    query_texts=question,
    n_results=1
    )
  if context["distances"][0][0] <= 0.4:
    input_text = "\n\nUser: " + question
  else:
    input_text = "System prompt:\n" + \
    "You are a helpful AI assistant who answers the user's question given some context.\n\n" + \
        "Context:\n " + context["documents"][0][0] + \
            "\n\nUser: " + question
  tokenized_input = tokenizer.encode(input_text, return_tensors="pt")
  attention_mask = torch.ones_like(tokenized_input)
  output = model.generate(input_ids = tokenized_input,
                          max_length=512, 
                          num_return_sequences=1, 
                          temperature=1.0, 
                          do_sample = True,
                          pad_token_id=tokenizer.eos_token_id)
  output_text = tokenizer.decode(output[0, tokenized_input.shape[-1]:], skip_special_tokens=True)
  print(output_text)
#   pattern = '/(?s).*:(.*)'
#   match = re.search(pattern, output_text)

#   if match:
#       assistant_text = match.group(1).strip()
#       print("\x1b[32mParsed Text: \x1b[0m")
#       print(assistant_text)
#   else:
#       print("No assistant text found.")

